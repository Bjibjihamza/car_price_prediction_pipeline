import os
import re
import requests
import unicodedata
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime

# Configuration des r√©pertoires
DATA_DIR = "../data/moteur"
IMAGES_DIR = os.path.join(DATA_DIR, "images")

# Cr√©er les r√©pertoires s'ils n'existent pas
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(IMAGES_DIR, exist_ok=True)

# Configuration de Selenium
options = Options()
options.add_argument("--headless")  # Ex√©cuter en arri√®re-plan
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
options.add_argument("--disable-blink-features=AutomationControlled")  # Contourner la d√©tection des bots
options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")


headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

def sanitize_filename(filename):
    """Nettoie un nom de fichier pour qu'il soit valide sur le syst√®me d'exploitation."""
    # Remplacer les caract√®res non-alphanum√©riques par des underscores
    filename = re.sub(r'[^\w\s-]', '_', filename)
    # Normaliser les caract√®res accentu√©s
    filename = unicodedata.normalize('NFKD', filename).encode('ASCII', 'ignore').decode('ASCII')
    # Remplacer les espaces par des underscores
    filename = re.sub(r'\s+', '_', filename)
    return filename

def download_image(url, folder_path, index):
    """T√©l√©charge une image √† partir d'une URL avec des en-t√™tes am√©lior√©s."""
    try:
        print(f"T√©l√©chargement de {url} vers {folder_path}")
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code == 200:
            file_extension = url.split('.')[-1]
            if '?' in file_extension:
                file_extension = file_extension.split('?')[0]
            if not file_extension or len(file_extension) > 5:
                file_extension = "jpg"  # Extension par d√©faut si probl√®me
            image_path = os.path.join(folder_path, f"image_{index}.{file_extension}")
            with open(image_path, 'wb') as f:
                f.write(response.content)
            print(f"‚úÖ Image enregistr√©e : {image_path}")
            return True
        else:
            print(f"‚ùå Erreur HTTP {response.status_code} pour {url}")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors du t√©l√©chargement de l'image {url}: {e}")
        return False

def scrape_detail_page(driver, url, ad_id, title, price_from_csv):
    """Scrape les d√©tails d'une annonce sp√©cifique, en utilisant le prix du CSV."""
    try:
        # Acc√©der √† la page de d√©tail
        driver.get(url)
        time.sleep(3)  # Attendre le chargement de la page
        
        # Cr√©er un dossier pour les images de cette annonce
        folder_name = f"{ad_id}_{sanitize_filename(title)}"
        folder_path = os.path.join(IMAGES_DIR, folder_name)
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)
            print(f"üìÇ Dossier cr√©√© : {folder_path}")
        
        # R√©cup√©rer les d√©tails du v√©hicule
        details = {}
        
        # 1. Prix - Utiliser la valeur du CSV au lieu de scraper
        details["Prix"] = price_from_csv
        
        # 2. Informations techniques dans les detail_line
        detail_lines = driver.find_elements(By.CLASS_NAME, "detail_line")
        
        for line in detail_lines:
            try:
                spans = line.find_elements(By.TAG_NAME, "span")
                if len(spans) >= 2:
                    key = spans[0].text.strip()
                    value = spans[1].text.strip()
                    
                    if "Kilom√©trage" in key:
                        details["Kilom√©trage"] = value
                    elif "Ann√©e" in key:
                        details["Ann√©e"] = value
                    elif "Boite de vitesses" in key:
                        details["Transmission"] = value
                    elif "Carburant" in key:
                        details["Type de carburant"] = value
                    elif "Date" in key:
                        details["Date de publication"] = value
                    elif "Puissance" in key:
                        details["Puissance fiscale"] = value
                    elif "Nombre de portes" in key:
                        details["Nombre de portes"] = value
                    elif "Premi√®re main" in key:
                        details["Premi√®re main"] = value
                    elif "V√©hicule d√©douan√©" in key:
                        details["D√©douan√©"] = value
            except Exception as e:
                print(f"Erreur lors de l'extraction d'une ligne de d√©tail: {e}")
        
        # 3. Description
        try:
            description_element = driver.find_element(By.CSS_SELECTOR, "div.options div.col-md-12")
            details["Description"] = description_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction description: {e}")
            details["Description"] = "N/A"
        
        # 4. Nom du vendeur
        try:
            seller_element = driver.find_element(By.XPATH, "//a[contains(@href, 'stock-professionnel') and .//i[contains(@class, 'icon-normal-megaphone')]]")
            details["Cr√©ateur"] = seller_element.text.strip() if seller_element.text.strip() else "N/A"
            
        except Exception as e:
            print(f"Erreur extraction vendeur: {e}")
            details["Cr√©ateur"] = "N/A"

        
        # 5. Ville
        try:
            city_element = driver.find_element(By.CSS_SELECTOR, "a[href*='ville']")
            details["Sector"] = city_element.text.strip()
        except Exception as e:
            print(f"Erreur extraction ville: {e}")
            details["Sector"] = "N/A"
        
        # 6. Images - M√©thode am√©lior√©e de t√©l√©chargement
        image_count = 0
        try:
            # Trouver les √©l√©ments d'image
            image_elements = driver.find_elements(By.CSS_SELECTOR, "img[data-u='image']")
            
            if not image_elements:
                print("‚ö†Ô∏è Aucune image trouv√©e sur la page. Essai d'une s√©lection alternative...")
                # Essayer une autre m√©thode de s√©lection
                image_elements = driver.find_elements(By.CSS_SELECTOR, ".swiper-slide img")
                
            print(f"Trouv√© {len(image_elements)} images potentielles")
            
            for index, img in enumerate(image_elements):
                img_url = img.get_attribute("src")
                if img_url and "http" in img_url:
                    success = download_image(img_url, folder_path, index + 1)
                    if success:
                        image_count += 1
                else:
                    print(f"URL d'image invalide: {img_url}")
            
            # Si toujours pas d'images, essayer de chercher dans le code source
            if image_count == 0:
                print("Recherche d'images dans le code source...")
                page_source = driver.page_source
                img_urls = re.findall(r'src=[\'"]([^\'"]*\.(?:jpg|jpeg|png|gif)(?:\?[^\'"]*)?)[\'"]', page_source)
                for index, img_url in enumerate(set(img_urls)):
                    if "http" in img_url and "thumb" not in img_url.lower():
                        success = download_image(img_url, folder_path, index + 1)
                        if success:
                            image_count += 1
            
            print(f"üì∏ {image_count} images t√©l√©charg√©es pour {title}")
        except Exception as e:
            print(f"Erreur lors de l'extraction des images: {e}")
        
        # Ajouter le nombre d'images t√©l√©charg√©es
        details["Nombre d'images"] = str(image_count)
        
        # Ajouter l'ID, le titre, l'URL et le dossier d'images
        details["ID"] = ad_id
        details["Titre"] = title
        details["URL de l'annonce"] = url
        details["Dossier d'images"] = folder_name
        
        return details
        
    except Exception as e:
        print(f"‚ùå Erreur lors du scraping de la page {url}: {e}")
        return {
            "ID": ad_id,
            "Titre": title,
            "URL de l'annonce": url,
            "Prix": price_from_csv,  # Inclure le prix du CSV m√™me en cas d'erreur
            "Erreur": str(e)
        }

def load_listings_file():
    """Charge le fichier de listings le plus r√©cent."""
    try:
        # Essayer d'abord de lire le pointeur vers le fichier le plus r√©cent
        pointer_file = os.path.join(DATA_DIR, "latest_listings_file.txt")
        if os.path.exists(pointer_file):
            with open(pointer_file, 'r') as f:
                latest_file = f.read().strip()
                if os.path.exists(latest_file):
                    print(f"‚úÖ Utilisation du fichier indiqu√© par le pointeur: {latest_file}")
                    return latest_file
                
        # Si pas de pointeur ou fichier inexistant, chercher le plus r√©cent CSV
        csv_files = [f for f in os.listdir(DATA_DIR) if f.startswith("moteur_ma_listings") and f.endswith(".csv")]
        if not csv_files:
            raise FileNotFoundError("Aucun fichier de listings trouv√© dans le r√©pertoire.")
        
        # Prendre le fichier le plus r√©cent
        latest_file = sorted(csv_files)[-1]
        latest_path = os.path.join(DATA_DIR, latest_file)
        print(f"‚úÖ Fichier de listings le plus r√©cent: {latest_path}")
        return latest_path
        
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du fichier de listings: {e}")
        return None

def main():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(DATA_DIR, f"moteur_ma_details.csv")
    
    # Charger le fichier de listings
    listings_file = load_listings_file()
    if not listings_file:
        print("‚ùå Impossible de continuer sans fichier de listings.")
        return
    
    # Charger les donn√©es du CSV
    try:
        listings_df = pd.read_csv(listings_file, encoding="utf-8-sig")
        print(f"üìä {len(listings_df)} annonces charg√©es depuis {listings_file}")
    except Exception as e:
        print(f"‚ùå Erreur lors de la lecture du fichier CSV: {e}")
        return
    
    # Initialiser le driver
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    
    try:
        # √âtape 1: Extraire les d√©tails pour chaque annonce
        print("\nüîé D√©marrage de l'extraction des d√©tails pour chaque annonce...")
        detailed_data = []
        
        for index, row in listings_df.iterrows():
            try:
                ad_id = row["ID"]
                title = row["Titre"]
                link = row["Lien"]
                price_from_csv = row.get("Prix", "N/A")  # R√©cup√©rer le prix du CSV
                
                print(f"[{index+1}/{len(listings_df)}] Scraping de l'annonce: {title} - Prix CSV: {price_from_csv}")
                
                if link and link != "N/A" and "http" in link:
                    # Extraire les d√©tails de la page, en transmettant le prix du CSV
                    details = scrape_detail_page(driver, link, ad_id, title, price_from_csv)
                    detailed_data.append(details)
                    
                    # Pause pour √©viter le blocage
                    time.sleep(2 + (index % 3))  # Pause variable entre 2 et 4 secondes
                else:
                    print(f"‚ùå Lien invalide pour l'annonce {ad_id}: {link}")
                    detailed_data.append({
                        "ID": ad_id,
                        "Titre": title,
                        "Prix": price_from_csv,  # Inclure le prix du CSV
                        "URL de l'annonce": link,
                        "Erreur": "Lien invalide"
                    })
            except Exception as e:
                print(f"‚ùå Erreur lors du traitement de l'annonce {index}: {e}")
        
        # √âtape 2: Convertir en DataFrame et enregistrer
        print("\nüíæ Pr√©paration et sauvegarde des donn√©es compl√®tes...")
        result_df = pd.DataFrame(detailed_data)
        
        # R√©organiser les colonnes
        columns_order = [
            "ID", "Titre", "Prix", "Date de publication", "Ann√©e", 
            "Type de carburant", "Transmission", "Kilom√©trage", 
            "Puissance fiscale", "Nombre de portes", "Premi√®re main", 
            "D√©douan√©", "Description", "Sector", "Cr√©ateur", 
            "URL de l'annonce", "Dossier d'images", "Nombre d'images"
        ]
        
        # Filtrer pour inclure seulement les colonnes pr√©sentes
        actual_columns = [col for col in columns_order if col in result_df.columns]
        result_df = result_df[actual_columns]
        
        # Enregistrer les donn√©es
        result_df.to_csv(output_file, index=False, encoding="utf-8-sig")
        print(f"‚úÖ Scraping d√©taill√© termin√© ! {len(result_df)} annonces trait√©es.")
        print(f"üìä Donn√©es enregistr√©es dans {output_file}")
        
        # Afficher des statistiques
        successful_images = sum(int(row.get("Nombre d'images", "0")) for _, row in result_df.iterrows())
        print(f"üìä Statistiques:")
        print(f"  - Annonces trait√©es: {len(result_df)}")
        print(f"  - Images t√©l√©charg√©es: {successful_images}")
        print(f"  - Moyenne d'images par annonce: {successful_images/len(result_df) if len(result_df) > 0 else 0:.1f}")
        
    except Exception as e:
        print(f"‚ùå Erreur globale: {e}")
    finally:
        # Fermer le navigateur
        driver.quit()
        print("üèÅ Programme termin√©.")

if __name__ == "__main__":
    main()